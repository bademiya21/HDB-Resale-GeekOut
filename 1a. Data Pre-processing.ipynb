{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process HDB Data\n",
    "This notebook loads the 4 datasets and combines them into one master dataset for use later. There are some transformations done to the columns of the data like combining columns to form a proper address and using OneMap APIs (Application Programming Interfaces) to find the associated postal codes (missing from HDB data). These postal codes will be used mostly during visualization. Rows that contain missing values are also dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the libraries needed to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the folder where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Transformations\n",
    "Load the 4 csv files downloaded from [here](https://data.gov.sg/dataset/resale-flat-prices) into Pandas dataframes. Compare their column headers to make sure all of the same column names. Load the first 5 rows to understand the data type and to ensure that they are all the same across the datasets. By the way, one of the files has an extra column compared to the other 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files in one-by-one and inspect columns\n",
    "df1 = pd.read_csv(DATA_DIR+'resale-flat-prices-based-on-approval-date-1990-1999.csv')\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the first 5 rows\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files in one-by-one and inspect columns\n",
    "df2 = pd.read_csv(DATA_DIR+'resale-flat-prices-based-on-approval-date-2000-feb-2012.csv')\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the first 5 rows\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files in one-by-one and inspect columns\n",
    "df3 = pd.read_csv(DATA_DIR+'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv')\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the first 5 rows\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files in one-by-one and inspect columns\n",
    "df4 = pd.read_csv(DATA_DIR+'resale-flat-prices-based-on-registration-date-from-jan-2015-onwards.csv')\n",
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the first 5 rows\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create missing column\n",
    "\n",
    "df4 contains an extra column compared to the other 3 - \"remaining_lease\". Pandas merging only works if all the columns of the dataframes have the same headers and the number of columns in all dataframes are the same. What is the meaning of this column? HDB flats are sold on a 99-year lease to the general public. What this means is that the value of the property at the end of the lease is deemed to be $0 and the government will take back the flats at the end of the lease for redevelopment purposes. As such, the number of years remaining on the lease agreement at the point of sale does affect the prices of HDB flats to a certain extent. Hence, it will be wise to keep this column for now instead of dropping it. \n",
    "\n",
    "However, this means that we need to create the same column in the other 3 dataframes so that they all have the same number and type of columns. Now, this column can be easily created in the other dataframes. There is a column which indicates the year in which the lease commenced - \"lease_commence_date\" and the year in which the flat was sold - \"month\". The \" \"remaining_lease\" can then be computed as follows:- 99 - (\"month\"[year] - lease_commence_date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But first, convert the datetime columns in all the dataframes to datetime format. At the moment, they are of type string\n",
    "df1['month'] = pd.to_datetime(df1['month'], format=\"%Y-%m\")\n",
    "df1['lease_commence_date'] = pd.to_datetime(df1['lease_commence_date'], format=\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do\n",
    "## Repeat the steps from the cell above for the remaining dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "df2['month'] = pd.to_datetime(df2['month'], format=\"%Y-%m\")\n",
    "df2['lease_commence_date'] = pd.to_datetime(df2['lease_commence_date'], format=\"%Y\")\n",
    "df3['month'] = pd.to_datetime(df3['month'], format=\"%Y-%m\")\n",
    "df3['lease_commence_date'] = pd.to_datetime(df3['lease_commence_date'], format=\"%Y\")\n",
    "df4['month'] = pd.to_datetime(df4['month'], format=\"%Y-%m\")\n",
    "df4['lease_commence_date'] = pd.to_datetime(df4['lease_commence_date'], format=\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the new column\n",
    "df1['remaining_lease'] = 99 - (df1['month'].dt.year - df1['lease_commence_date'].dt.year)\n",
    "df2['remaining_lease'] = 99 - (df2['month'].dt.year - df2['lease_commence_date'].dt.year)\n",
    "df3['remaining_lease'] = 99 - (df3['month'].dt.year - df3['lease_commence_date'].dt.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the new column\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the dataframes. Can you figure the function from pandas that can do this? You can refer to the pandas documentation [here](https://pandas.pydata.org/pandas-docs/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do\n",
    "## Write a line of code to merge the dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "df = pd.concat([df1,df2,df3,df4], sort=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the dataframe in ascending order of dates when the flats were sold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sort the dataframe by month column in ascending order\n",
    "df = df.sort_values(['month'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some other minor cleaning of the data\n",
    "After merging data, it is always a good practice to check columns (especially string/character type columns) to make sure the values are consistent. Here, check the \"flat_model\", \"flat_type\", \"storey_range\" and \"town\" columns for any inconsistent values and correct them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat_model column\n",
    "values = list(set(df['flat_model'].values.tolist()))\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flat_model'] = df.flat_model.str.lower()\n",
    "# Standardize repeated values of Multi-Generation and Premium Apartment\n",
    "df['flat_model'] = df['flat_model'].replace(['multi generation','premium apartment.'], ['multi-generation','premium apartment'])\n",
    "values = list(set(df['flat_model'].values.tolist()))\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flat_type column\n",
    "values = list(set(df['flat_type'].values.tolist()))\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"flat_type\" has variables that are actually the same one variable but are represented differently. Can you correct them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do\n",
    "## Write the code to correct the variable values\n",
    "## HINT: Refer to few cells above\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "df['flat_type'] = df['flat_type'].replace(['MULTI GENERATION'], ['MULTI-GENERATION'])\n",
    "values = list(set(df['flat_type'].values.tolist()))\n",
    "values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storey_range column\n",
    "values = list(set(df['storey_range'].values.tolist()))\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# town column\n",
    "values = list(set(df['town'].values.tolist()))\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new column from existing columns\n",
    "The address of each flat is split between columns - block number is in \"block\" column and street name is in \"street_name\" column. For ease, it would be preferable to combine these 2 columns into 1 and drop the other 2. Later, this column will be used to find the corresponding postal codes which is missing from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine block and street name column to form a new column, address. Drop the block and street name columns\n",
    "df['Address'] = (df['block'] + ' ' + df['street_name']).str.lower()\n",
    "df = df.drop(['block','street_name'], axis = 1)\n",
    "print(df.head())\n",
    "print(\"Total number of rows in dataset: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Postal Code column\n",
    "Now, from the newly created Address column, the postal codes can be scraped from OneMap APIs by passing the address values to the site and extracting the postal code values from the JSON (JavaScript Object Notation) responses returned.\n",
    "\n",
    "First, we need to find the unique list of addresses for searching. The dataset contains repeated rows of same addresses as the flats and the blocks could appear in multiple transactions over the years. Getting the unique values reduces the number of API calls needed and avoids redundancy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get postal code for addresses in dataset\n",
    "address_list = list(set(df.Address.values.tolist()))\n",
    "print(\"Number of unique addresses: {}\".format(len(address_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the unique addresses are obtained, call the OneMap API sequentially to search the database for matching address and return the JSON response. If there is a valid response returned, check to see that the JSON data is not empty. If empty, indicate a None response for postal code. Otherwise, extract the postal code and push it into a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = \"https://developers.onemap.sg/commonapi/search\"\n",
    "\n",
    "postal_code_list = [] # To store postal codes returned\n",
    "latitude_list = []\n",
    "longitude_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for address in tqdm.tqdm_notebook(address_list):\n",
    "    query = {'searchVal': address,\n",
    "             'returnGeom': 'Y',\n",
    "             'getAddrDetails' : 'Y'\n",
    "            }\n",
    "    \n",
    "    # If an okay response is not returned, keep retrying the API call until it is sucessful. If successful,\n",
    "    # store the returned postal code and exit the loop. Move to the next address in the list.\n",
    "    status_check = 0\n",
    "    while status_check != 200:\n",
    "        response = requests.get(search_url, params=query)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if (len(data['results'])>0) and (str(data['results'][0]['POSTAL']) != 'NIL'):\n",
    "                postal_code_list.append(str(data['results'][0]['POSTAL']))\n",
    "                latitude_list.append(data['results'][0]['LATITUDE'])\n",
    "                longitude_list.append(data['results'][0]['LONGITUDE'])\n",
    "            else:\n",
    "                postal_code_list.append(None) # No postal code returned\n",
    "                latitude_list.append(None) # No latitude returned\n",
    "                longitude_list.append(None) # No longitude returned\n",
    "            status_check = 200\n",
    "        else:\n",
    "            print(\"API Call unsuccessful. Retrying...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataframe of address and postal codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_postal = pd.DataFrame(\n",
    "    {'Address': address_list,\n",
    "     'Postal Code': postal_code_list,\n",
    "     'Latitude': latitude_list,\n",
    "     'Longitude' : longitude_list\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the above dataframe into the master dataframe to create the column of postal codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge postal codes to main dataframe\n",
    "df.set_index('Address', inplace=True)\n",
    "df_add_postal.set_index('Address', inplace=True)\n",
    "df = df.join(df_add_postal, how='left').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping rows with missing values\n",
    "The API calls returned some empty postal code values for some addresses. The reasons for the NIL return could be due to the flats/blocks no longer existing i.e. torn down, redeveloped, renamed etc. or the database of OneMap being not up-to-date. It is best to remove these rows of data from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of rows in dataset before removing rows with NA, NULL or None values: {}\".format(len(df)))\n",
    "df = df.dropna()\n",
    "print(\"Total number of rows in dataset after removing rows with NA, NULL or None values: {}\".format(len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset with postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR+'HDB_Resale_Data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MRT Station Locations into Pandas Dataframe (for Tableau Visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locations of MRT stations around Singapore can be downloaded from [here](https://www.mytransport.sg/content/dam/datamall/datasets/Geospatial/TrainStation.zip). Inside the folder, there are multiple files but of importance, there are 2 - the files that end with *.shp* and *.dbf* extensions. Copy both files out into the DATA_DIR folder after unzipping. The next 2 cells does some manipulation of the data so that it is readable in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapefile #the pyshp module\n",
    "\n",
    "#read file, parse out the records and shapes\n",
    "mrt_shapefile_path = DATA_DIR + 'MRTLRTStnPtt.shp'\n",
    "mrt_sf = shapefile.Reader(mrt_shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab the shapefile's field names (omit the first psuedo field)\n",
    "fields = [x[0] for x in mrt_sf.fields][1:]\n",
    "records = mrt_sf.records()\n",
    "x = [s.points[0][0] for s in mrt_sf.shapes()]\n",
    "y = [s.points[0][1] for s in mrt_sf.shapes()]\n",
    "\n",
    "#write the records into a dataframe\n",
    "mrt_df = pd.DataFrame(columns=fields, data=records)\n",
    "\n",
    "#add the coordinates data to columns called \"coords.x\" and \"coords.y\"\n",
    "mrt_df = mrt_df.assign(coords_x=x,coords_y=y)\n",
    "mrt_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the location coordinates of the station are in SVY21 format (which is the typical location cooordinates for Singapore addresses). In order to make it comparable to the cooordinates system of the HDB flat addresses found earlier, we need to do some conversion using OneMap APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_url = \"https://developers.onemap.sg/commonapi/convert/3414to4326\"\n",
    "\n",
    "latitude_list = []\n",
    "longitude_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the example a few cells above, can you recreate the *for* loop to call the API to get the latitudes and longitudes? HINT: You can zip *mrt_df['coords_x']* and *mrt_df['coords_y']* inside the for iterator as *zip(mrt_df['coords_x'],mrt_df['coords_y'])*. You can also look at the OneMap API documentation [here](https://docs.onemap.sg/#3414-svy21-to-4326-wgs84) to see what are the query parameters needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To do\n",
    "## Write a for loop to call the API to get the latitudes and longitudes\n",
    "## You can copy the for loop from the previous API call cell and modify accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "with tqdm.tqdm(total=len(mrt_df)) as pbar:\n",
    "    for x,y in zip(mrt_df['coords_x'],mrt_df['coords_y']):\n",
    "        query = {'X': x,\n",
    "                 'Y': y\n",
    "                }\n",
    "\n",
    "        # If an okay response is not returned, keep retrying the API call until it is sucessful. If successful,\n",
    "        # store the returned coordinates and exit the loop. Move to the next address in the list.\n",
    "        status_check = 0\n",
    "        while status_check != 200:\n",
    "            response = requests.get(convert_url, params=query)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if len(data)>0:\n",
    "                    latitude_list.append(data['latitude'])\n",
    "                    longitude_list.append(data['longitude'])\n",
    "                else:\n",
    "                    latitude_list.append(None) # No latitude returned\n",
    "                    longitude_list.append(None) # No longitude returned\n",
    "                status_check = 200\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                print(\"API Call unsuccessful. Retrying...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the latitude and longitudes of the MRT stations found to the dataframe and drop the columns with the SVY21 coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_df['latitude'] =  latitude_list\n",
    "mrt_df['longitude'] =  longitude_list\n",
    "mrt_df = mrt_df.drop(['OBJECTID','coords_x','coords_y'],axis=1)\n",
    "mrt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_df.to_csv(DATA_DIR+'MRT_Locations.csv', index=False) # Saving the mrt stations location data to a csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find nearest MRT station to each address in HDB resale transaction data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDB resale data contains 781 030 rows of data and the mrt locations data contains 183 rows of data. Now, we want to find what is the nearest MRT station to each address in the resale data. By brute force, for each address, we need to compute 183 distances and then select the minimum. For the entire dataset, this requires 781 030 x 183 = 142,928,490 operations! This is a very time-consuming process and may take almost a day to run. Here, we present a trick using a data structure known as trees to speed up and simplify the search operations. Specifically, we are using a __vantage-point (VP) tree__ to find the nearest MRT station.\n",
    "![Vantage Point](images/VP.png)\n",
    "\n",
    "A VP tree is a metric tree that segregates data in a metric space by choosing a position in the space (\"vantage point\") and partioning the data points into 2 parts: those points that are nearer to the vantage point than a threshold and those points that are not. By recursively applying this procedure to partition the data into smaller and smaller sets, a tree data structure is created where neighbours in the tree are likely to be neighbours in space. Here, the distance metric used is the __geodesic distance__ which is the shortest distance on the surface of an ellipsoidal model of the earth. This distance computation is provided by the *geopy* package. Once the tree is computed, the nearest lat-lon to the lat-lon of interest is extracted by finding the closest neighbour and then, the corresponding mrt station name is extracted and tagged to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vptree\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gps_distance(p1,p2):\n",
    "    return distance.distance((p1[0],p1[1]),(p2[0],p2[1])).km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_point(row):\n",
    "    point = tree.get_nearest_neighbor(row[['Latitude','Longitude']])\n",
    "    return [point[0],point[1][0],point[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vantage-point(VP) tree in O(n log n) time complexity\n",
    "tree = vptree.VPTree(mrt_df[['latitude','longitude']].values,gps_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the latitude-longitude information of the HDB flats from the data. Now, there are a lot of duplicate entries which can be removed. This reduces the number of search operations needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lat_lon = df[['Latitude','Longitude']]\n",
    "print(\"Number of lat-lon (include repeats): {}\".format(len(df_lat_lon)))\n",
    "df_lat_lon = df_lat_lon.drop_duplicates()\n",
    "print(\"Number of unique lat-lon: {}\".format(len(df_lat_lon)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for comparison sake: Creating a VP-tree is a *O(n log n)* operation where n here is 183. Therefore, creating a tree takes about 414 operations. Searching one point is a *O(log n)* operation which is about 2 operations. So for 8754 data points, about 17 508 operations. So, total number of operations:  about 17 508 vs 142,928,490 operations by brute force method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the nearest MRT location in data using the VP tree\n",
    "df_lat_lon[['Distance to MRT','MRT_Latitude','MRT_Longitude']] = df_lat_lon.apply(find_nearest_point,axis=1,result_type = 'expand')\n",
    "df_lat_lon = pd.merge(df_lat_lon,mrt_df,left_on=['MRT_Latitude','MRT_Longitude'],right_on=['latitude','longitude'],how='left').drop(['latitude','longitude'],axis=1)\n",
    "df_lat_lon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally merge the locations into the main data\n",
    "df = pd.merge(df, df_lat_lon, on = ['Latitude','Longitude'], how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset for Tableau visualization\n",
    "Now that the dataset with MRT stations has been processed, save the dataset for use later for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR+'HDB_Resale_Data_mrt_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
